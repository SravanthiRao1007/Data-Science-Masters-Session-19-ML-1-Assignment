{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Science Masters Session 19 Machine Learning 1 Assignment Sravanthi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer: The three stages to build a hypothesis or model in machine learning are \\n\\n1.Model building that involves Data Preparation, Training and Test Set Generation, Algorithm Training\\n2.Model testing that involves Prediction and Evaluation of Test Data\\n3.Applying the model that involves Deployment and Monitoring\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Assignment Q 19.1 : What are the three stages to build the hypotheses or model in machine learning?\n",
    "\n",
    "\"\"\"\n",
    "Answer: The three stages to build a hypothesis or model in machine learning are \n",
    "\n",
    "1.Model building that involves Data Preparation, Training and Test Set Generation, Algorithm Training\n",
    "2.Model testing that involves Prediction and Evaluation of Test Data\n",
    "3.Applying the model that involves Deployment and Monitoring\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer:\\nIn supervised learning we gather labeled data and we apply data preprocessing and cleaning on the data sets. Then we divide the  \\nset of samples into training sets and test sets. 80:20 ratio is considered as a good proportion for train-test-split.\\nThen we again divide the training data into further new training data sets and validation data sets.\\nWe choose one of the suitable algorithms.Then we train machine learning model on new training data sets and \\ntune+evaluate the model using validation data. Once the model is performing well on validation data\\nthen we test the model over test set. if the prediction is not accurate to \\nour expectation, we repeat the steps.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment Q 19.2 What is the standard approach to supervised learning?\n",
    "\n",
    "\"\"\"\n",
    "Answer:\n",
    "In supervised learning we gather labeled data and we apply data preprocessing and cleaning on the data sets. Then we divide the  \n",
    "set of samples into training sets and test sets. 80:20 ratio is considered as a good proportion for train-test-split.\n",
    "Then we again divide the training data into further new training data sets and validation data sets.\n",
    "We choose one of the suitable algorithms.Then we train machine learning model on new training data sets and \n",
    "tune+evaluate the model using validation data. Once the model is performing well on validation data\n",
    "then we test the model over test set. if the prediction is not accurate to \n",
    "our expectation, we repeat the steps.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer:\\nTraining set is a data set used to train the model. Specific features are picked up from the training set for training purpose.\\nThese features are then incorporated into the model.\\n\\nTest set is a data set used to measure how well the model performs at making predictions on the test data based on the training.\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment Q 19.3 What is Training set and Test set?\n",
    "\n",
    "\"\"\"\n",
    "Answer:\n",
    "Training set is a data set used to train the model. Specific features are picked up from the training set for training purpose.\n",
    "These features are then incorporated into the model.\n",
    "\n",
    "Test set is a data set used to measure how well the model performs at making predictions on the test data based on the training.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAnswer:\\nThe general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm\\nin order to improve robustness over a single model. Ensemble learning is used when you build component classifiers that are \\nmore accurate and independent from each other.\\n\\nBagging is a method in ensemble for improving unstable estimation or classification schemes.Bagging is used when the goal \\nis to reduce the variance of a decision tree classifier. Here the objective is to create several \\nsubsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their \\ndecision trees.\\n\\nBoosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting is to \\ntrain weak learners sequentially, each trying to correct its predecessor. It is used sequentially to reduce the bias of \\nthe combined model.\\n\\nBoosting and Bagging both can reduce errors by reducing the variance term.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment Q 19.4 What is the general principle of an ensemble method and what is bagging and boosting in ensemble method?\n",
    "\n",
    "\"\"\"\n",
    "Answer:\n",
    "The general principle of an ensemble method is to combine the predictions of several models built with a given learning algorithm\n",
    "in order to improve robustness over a single model. Ensemble learning is used when you build component classifiers that are \n",
    "more accurate and independent from each other.\n",
    "\n",
    "Bagging is a method in ensemble for improving unstable estimation or classification schemes.Bagging is used when the goal \n",
    "is to reduce the variance of a decision tree classifier. Here the objective is to create several \n",
    "subsets of data from training sample chosen randomly with replacement. Each collection of subset data is used to train their \n",
    "decision trees.\n",
    "\n",
    "Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting is to \n",
    "train weak learners sequentially, each trying to correct its predecessor. It is used sequentially to reduce the bias of \n",
    "the combined model.\n",
    "\n",
    "Boosting and Bagging both can reduce errors by reducing the variance term.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nAnswer:\\n\\nThere are few methods to prevents from overfitting\\n\\n1.Cross-validation : a model is usually given a dataset of a known data on which training (training data set) is run and a \\ndataset of unknown data against which the model is tested. The idea of cross validation is to define a dataset to “test” \\nthe model in the training phase\\n\\n2.Train with more data : By using a lot of data over fitting can be avoided, over fitting happens relatively as you have a \\nsmall dataset, and you try to learn from it. But if you have a small database and you are forced to come with a model based \\non that\\n\\n3.Remove features\\n\\n4.Early stopping :  Early stopping is a form of regularization used to avoid overfitting when training a learner\\nwith an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training \\ndata with each iteration. \\nUp to a point, this improves the learner's performance on data outside of the training set. Past that \\n\\n5.Ensembling(Bagging and boosting)\\n\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assignment Q 19.5 How can you avoid overfitting ?\n",
    "\n",
    "\"\"\"\n",
    "Answer:\n",
    "\n",
    "There are few methods to prevents from overfitting\n",
    "\n",
    "1.Cross-validation : a model is usually given a dataset of a known data on which training (training data set) is run and a \n",
    "dataset of unknown data against which the model is tested. The idea of cross validation is to define a dataset to “test” \n",
    "the model in the training phase\n",
    "\n",
    "2.Train with more data : By using a lot of data over fitting can be avoided, over fitting happens relatively as you have a \n",
    "small dataset, and you try to learn from it. But if you have a small database and you are forced to come with a model based \n",
    "on that\n",
    "\n",
    "3.Remove features\n",
    "\n",
    "4.Early stopping :  Early stopping is a form of regularization used to avoid overfitting when training a learner\n",
    "with an iterative method, such as gradient descent. Such methods update the learner so as to make it better fit the training \n",
    "data with each iteration. \n",
    "Up to a point, this improves the learner's performance on data outside of the training set. Past that \n",
    "\n",
    "5.Ensembling(Bagging and boosting)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
